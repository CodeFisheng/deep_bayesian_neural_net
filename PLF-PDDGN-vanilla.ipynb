{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting parameters .............\n",
      "loading data ...........\n",
      "reach max file number\n",
      "Construct Neural Nets\n",
      "Start\n"
     ]
    }
   ],
   "source": [
    "############################# this code unfinished #######################\n",
    "# seperately apply marginal interal sampling on different households\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl\n",
    "from numpy import random, histogram2d, diff\n",
    "from scipy.interpolate import interp2d\n",
    "#%matplotlib inline\n",
    "# class County:\n",
    "#     def __init__(self,parsename):\n",
    "#         self.parsename = parsename\n",
    "#         dataframe = xls.parse(parsename)\n",
    "#         self.data = dataframe\n",
    "#     def disp_all(self):\n",
    "#         print self.dataframe\n",
    "#     def get_all(self):\n",
    "#         return self.dataframe\n",
    "# xls = pd.ExcelFile('No-drybulb-dewpoint-short-dataset-CT.xlsx',header = None)\n",
    "# Zonal = []\n",
    "# Zonal.append(County('CT'))\n",
    "# ZonalNum = 1\n",
    "\n",
    "print \"setting parameters .............\"\n",
    "num_epochs = 20000 # training epoches for each customer samples\n",
    "num_realisations = 5000\n",
    "out_thresh = num_epochs - 100\n",
    "day_steps_f = 48\n",
    "#val_rate_f = 0.15\n",
    "test_batch_size_f = 28*day_steps_f # days of a batch\n",
    "valid_batch_size_f = 28*day_steps_f\n",
    "train_batch_size_f = 28*day_steps_f\n",
    "n_output_f = 1\n",
    "n_hidden_f_1 = 100\n",
    "n_hidden_f_2 = 100\n",
    "n_hidden_f_3 = 100\n",
    "n_hidden_f_4 = 100\n",
    "tao_f = 0.1\n",
    "gap_test_f = 10\n",
    "batch_size_f = test_batch_size_f # in this version, batch_size set same\n",
    "preserve_f = 0#16114 ## amount of first time points without complete features\n",
    "_dropout_train = 0.5\n",
    "_dropout_test = 1.0\n",
    "ZonalNum = 1\n",
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "\n",
    "\n",
    "\n",
    "############################||||||||||||||||||||||||||data loading\n",
    "\n",
    "print \"loading data ...........\"\n",
    "#ISO_name = 'No-drybulb-dewpoint-short-dataset-CT.csv'\n",
    "#HOME_name = 'data/MAC005540.csv'\n",
    "count = 0\n",
    "hid = 0\n",
    "for root, dirs, filenames in os.walk('./data/'):\n",
    "    for fname in filenames:\n",
    "        dbslice = pd.read_csv('./data/' + fname)\n",
    "        if count < hid:\n",
    "            count = count +1\n",
    "            continue;\n",
    "        if count == hid:\n",
    "            xls = dbslice\n",
    "        else:\n",
    "            xls = pd.concat([xls,dbslice], axis = 0)\n",
    "        count = count + 1\n",
    "        if count > hid:\n",
    "            print 'reach max file number'\n",
    "            break;\n",
    "xls.shape\n",
    "rows_f = xls.shape[0]\n",
    "columns_f = xls.shape[1]\n",
    "database_f = np.array(xls)\n",
    "#np.random.shuffle(database_f)\n",
    "for i in range(rows_f):\n",
    "    for j in range(columns_f):\n",
    "        database_f[i,j] = np.float(database_f[i,j])\n",
    "totalen_f = rows_f\n",
    "#print database_f[:,0]\n",
    "n_input_f = columns_f - 1\n",
    "data_norm = np.max(database_f, axis = 0)\n",
    "database_f = database_f/data_norm\n",
    "#print totalen_f\n",
    "db_f = database_f\n",
    "#print db_f\n",
    "#define id arrays\n",
    "test_id_f = np.array(test_batch_size_f)\n",
    "valid_id_f = np.array(2*valid_batch_size_f)\n",
    "train_id_f = np.array(totalen_f - test_batch_size_f - valid_batch_size_f)\n",
    "\n",
    "#give values to id arrays\n",
    "rang = range(preserve_f, totalen_f - test_batch_size_f)\n",
    "valid_id_f = rd.sample(rang,2*valid_batch_size_f)\n",
    "test_id_f = np.array(range(totalen_f - test_batch_size_f,totalen_f))\n",
    "train_id_f = set(range(preserve_f, totalen_f - test_batch_size_f)) - set(valid_id_f)\n",
    "\n",
    "#sort three id array\n",
    "valid_id_f = np.sort(valid_id_f)\n",
    "test_id_f = np.sort(test_id_f)\n",
    "train_id_f = np.array(list(train_id_f))\n",
    "def train_data_gen():\n",
    "    X = np.zeros((train_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y = np.zeros((train_batch_size_f,ZonalNum,n_output_f))\n",
    "    count = 0\n",
    "    rang = range(0,train_id_f.shape[0])\n",
    "    train_rd = rd.sample(rang,train_batch_size_f)\n",
    "    train_rd = np.sort(train_rd)\n",
    "    for i in train_rd:\n",
    "        j = train_id_f[i]\n",
    "        Y[count] = db_f[j,:1]\n",
    "        X[count] = db_f[j,1:]\n",
    "        count = count + 1\n",
    "    X = X.astype(np.float32)\n",
    "    Y = Y.astype(np.float32)\n",
    "    return (X,Y)\n",
    "\n",
    "def valid_data_gen():\n",
    "    X = np.zeros((train_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y = np.zeros((train_batch_size_f,ZonalNum,n_output_f))\n",
    "    count = 0\n",
    "    rang = range(0,valid_id_f.shape[0])\n",
    "    valid_rd = rd.sample(rang,train_batch_size_f)\n",
    "    valid_rd = np.sort(valid_rd)\n",
    "    for i in valid_rd:\n",
    "        j = valid_id_f[i]\n",
    "        Y[count] = db_f[j,:1]\n",
    "        X[count] = db_f[j,1:]\n",
    "        count = count + 1\n",
    "    X = X.astype(np.float32)\n",
    "    Y = Y.astype(np.float32)\n",
    "    return (X,Y)\n",
    "\n",
    "def test_data_gen():\n",
    "    X = np.zeros((test_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y = np.zeros((test_batch_size_f,ZonalNum,n_output_f))\n",
    "    count = 0\n",
    "    for i in test_id_f:\n",
    "        Y[count] = db_f[i,:1]\n",
    "        X[count] = db_f[i,1:]\n",
    "        count = count + 1\n",
    "    X = X.astype(np.float32)\n",
    "    Y = Y.astype(np.float32)\n",
    "    return (X,Y)\n",
    "\n",
    "print 'Construct Neural Nets'\n",
    "_X_f = tf.placeholder(tf.float32, [None, ZonalNum, n_input_f])\n",
    "_Y_f = tf.placeholder(tf.float32, [None, ZonalNum, n_output_f])\n",
    "_Dropout_f = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# Create model\n",
    "def MLP(x, _dropout, weights, biases):\n",
    "\n",
    "    x = tf.reshape(x, [-1, n_input_f])\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    x = tf.nn.dropout(x, _dropout)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1,_dropout)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2,_dropout)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "    layer_3 = tf.nn.dropout(layer_3,_dropout)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.sigmoid(layer_4)\n",
    "    # Output layer with linear activation\n",
    "    result = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    result = tf.nn.sigmoid(result)\n",
    "    return result\n",
    "# MLP\n",
    "weights_f = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input_f, n_hidden_f_1]), name = \"wf1\"),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_f_1, n_hidden_f_2]), name = \"w_f_2\"),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_f_2, n_hidden_f_3]), name = \"w_f_3\"),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_f_3, n_hidden_f_4]), name = \"w_f_4\"),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_f_4, n_output_f]), name = \"w_o\")\n",
    "}\n",
    "biases_f = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_f_1]), name = \"b_f_1\"),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_f_2]), name = \"b_f_2\"),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_f_3]), name = \"b_f_3\"),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_f_4]), name = \"b_f_4\"),\n",
    "    'out': tf.Variable(tf.random_normal([n_output_f]), name = \"b_o\")\n",
    "}\n",
    "pred_f = MLP(_X_f, _Dropout_f, weights_f, biases_f)\n",
    "reshaped_results_f = tf.reshape(_Y_f, [-1])\n",
    "reshaped_outputs_f = tf.reshape(pred_f, [-1])\n",
    "#coef = 0.0001\n",
    "#closs = coef*tf.nn.l2_loss(weights['h1']) + coef*tf.nn.l2_loss(weights['h2']) + coef*tf.nn.l2_loss(weights['h3']) + coef*tf.nn.l2_loss(weights['out'])\n",
    "cost_f = tf.reduce_mean(tf.pow(reshaped_results_f - reshaped_outputs_f,2))\n",
    "#cost = tf.nn.l2_loss(reshaped_results-reshaped_outputs)\n",
    "optimizer_f = tf.train.AdamOptimizer(learning_rate=0.005, beta1 = 0.8, beta2 = 0.7).minimize(cost_f)\n",
    "\n",
    "def maxe(predictions, targets):\n",
    "    return max(abs(predictions-targets))\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def mape(predictions, targets):\n",
    "    return np.mean(abs(predictions-targets)/targets)\n",
    "\n",
    "print \"Start\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    }
   ],
   "source": [
    "outlist = []\n",
    "realist = []\n",
    "samples = []\n",
    "samples_res = []\n",
    "features = []\n",
    "uncertainties = []\n",
    "kind = 0\n",
    "time1 = time.time()\n",
    "# generate test data\n",
    "test_x,test_y = test_data_gen()\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "reg = 0                        #,\"wf2\":w_f_2,\"wf3\":w_f_3,\"wf4\":w_f_4,\"wo\":w_o,\"bf1\":b_f_1,\"bf2\":b_f_2,\"bf3\":b_f_3,\"bf4\":b_f_4,\"bo\":b_o})\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Create a summary to monitor cost function\n",
    "    #tf.scalar_summary(\"loss\", cost_f)\n",
    "    # Merge all summaries to a single operator\n",
    "    #merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "    #summary_writer = tf.train.SummaryWriter('path/to/logs', graph_def=sess.graph_def)\n",
    "\n",
    "    #initialize all variables in the model\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver({\"wf1\":weights_f['h1'],\"wf2\":weights_f['h2'],\"wf3\":weights_f['h3'],\"wf4\":weights_f['h4'],\"wfo\":weights_f['out'],\"bf1\":biases_f['b1'],\"bf2\":biases_f['b2'],\"bf3\":biases_f['b3'],\"bf4\":biases_f['b4'],\"bfo\":biases_f['out']})\n",
    "    \n",
    "    #for k in range(num_epochs):\n",
    "    #    #print traindays\n",
    "    #    #     #print 'Training'\n",
    "    #    if rd.random() < 2*valid_batch_size_f/totalen_f:\n",
    "    #        X,Y = valid_data_gen()\n",
    "    #        cs, _ = sess.run([cost_f,optimizer_f], feed_dict = {_X_f:X, _Y_f:Y, _Dropout_f:_dropout_train})\n",
    "    #    else:\n",
    "    #        X,Y = train_data_gen()\n",
    "    #        cs, _ = sess.run([cost_f,optimizer_f],feed_dict={_X_f:X,_Y_f:Y,_Dropout_f: _dropout_train})\n",
    "    #        #summary2 = sess.run([cost_f,optimizer_f, merged_summary_op],feed_dict={_X_f:X,_Y_f:Y,_Dropout_f: _dropout_train})\n",
    "    #    if k % 1000 == 0:\n",
    "    #        print \"Iter \" + str(k) + \" ---- Process: \" + \"{:.2f}\".format(100*float(k)/float(num_epochs)) + \"%, loss = \"+\"{:.4f}\".format(100*np.sqrt(cs))+\"%\"\n",
    "            #summary_writer.add_summary(summary, k)\n",
    "            #summary_writer.add_summary\n",
    "        # if (k >= out_thresh) & (k % gap_test_f == 0):\n",
    "        #     #print test_x\n",
    "        #     err, reshaped_pred, reshaped_res = sess.run([cost_f, reshaped_outputs_f, reshaped_results_f],feed_dict = {_X_f:test_x,_Y_f:test_y,_Dropout_f: _dropout_test} )\n",
    "        #     print \"RMSE = \" + str(np.sqrt(err))\n",
    "        #     outlist.append(reshaped_pred)\n",
    "        #     realist.append(reshaped_res)\n",
    "        #     kind = kind + 1\n",
    "    #saver.save(sess, \"models/PDDGN-vanilla-20000.ckpt\")#%HOME_name[10:14])\n",
    "    saver.restore(sess, \"models/PDDGN-vanilla-20000.ckpt\")#%HOME_name[10:14])\n",
    "    # saver.save(sess, \"ISO_model_%d.ckpt\"%num_epochs)\n",
    "    # saver.restore(sess, \"ISO_model_%d.ckpt\"%num_epochs)\n",
    "    print 'Testing'\n",
    "    ######################################### resulting for load forecasting\n",
    "    for k in range(num_realisations):\n",
    "        reshaped_pred, reshaped_res = sess.run([reshaped_outputs_f, reshaped_results_f], feed_dict = {_X_f:test_x,_Y_f:test_y,_Dropout_f: _dropout_train})\n",
    "        samples.append(reshaped_pred)\n",
    "        samples_res.append(reshaped_res)\n",
    "\n",
    "\n",
    "samples = np.array(samples)\n",
    "samples = np.sort(samples, axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.03927823,  0.0234288 ,  0.02326257, ...,  0.04633704,\n",
      "        0.04634558,  0.04168561]), array([ 0.04307969,  0.02635265,  0.02610487, ...,  0.04942425,\n",
      "        0.04962503,  0.04487384]), array([ 0.0454831 ,  0.0290134 ,  0.02880855, ...,  0.0520699 ,\n",
      "        0.05224974,  0.04694724]), array([ 0.04751408,  0.03262831,  0.03222943, ...,  0.05468913,\n",
      "        0.05467962,  0.04862734]), array([ 0.04937379,  0.03863078,  0.03849895, ...,  0.05736958,\n",
      "        0.0572061 ,  0.05025164]), array([ 0.05123213,  0.04458775,  0.04449849, ...,  0.06012069,\n",
      "        0.05991   ,  0.05199754]), array([ 0.05314808,  0.04846102,  0.04835307, ...,  0.06327936,\n",
      "        0.06291068,  0.0536008 ]), array([ 0.05544178,  0.05171509,  0.05157577, ...,  0.06683889,\n",
      "        0.06655815,  0.0557355 ]), array([ 0.05871911,  0.05552023,  0.05504115, ...,  0.07168816,\n",
      "        0.07149979,  0.05887526])]\n",
      "10.633572741\n",
      "16.0826563469\n",
      "20.6532612306\n",
      "24.4061137406\n",
      "27.5415159273\n",
      "29.5705628102\n",
      "30.536358924\n",
      "30.2776556199\n",
      "29.09687458\n",
      "24.3109524356\n"
     ]
    }
   ],
   "source": [
    "Plist = []\n",
    "for i in range(0,9):\n",
    "\tind = 10*(i+1)\n",
    "\tPlist.append(np.percentile(samples, ind, axis = 0))\n",
    "print Plist\n",
    "def pinball_loss(A,B,tao):\n",
    "\tcost = 0.0\n",
    "\tA = A.reshape([-1])\n",
    "\tB = B.reshape([-1])\n",
    "\tfor i in range(A.shape[0]):\n",
    "\t    if A[i]-B[i]>=0:\n",
    "\t        tmp = (A[i]-B[i])*(tao)\n",
    "\t    else:\n",
    "\t        tmp = (B[i]-A[i])*(1.0-tao)\n",
    "\t    cost = tmp+cost\n",
    "\t#print cost\n",
    "\t#print ncost_s3\n",
    "\treturn cost\n",
    "taolist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "nloss_pb_total = 0.0\n",
    "loss_pb_total = 0.0\n",
    "for t in xrange(0,9):\n",
    "\ttao = taolist[t]\n",
    "\tloss_pb = pinball_loss(samples_res[0], Plist[t], tao)\n",
    "\t#print 'tao = %.4f, pb = %.4f'%(tao,loss_pb)\n",
    "\tprint loss_pb\n",
    "\tloss_pb_total = loss_pb_total + loss_pb\n",
    "#print \"dropout = %.4f, loss_pb_total = %.4f\"%(_dropout_train, loss_pb_total)\n",
    "print loss_pb_total/9\n",
    "#DataFrame(pinballist).to_csv(\"PINBALL-onelayer.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_pb_total =  24.3109524356\n",
      "Visualization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codefisheng/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:32: FutureWarning: \n",
      "The default value for 'return_type' will change to 'axes' in a future release.\n",
      " To use the future behavior now, set return_type='axes'.\n",
      " To keep the previous behavior and silence this warning, set return_type='dict'.\n"
     ]
    }
   ],
   "source": [
    "print \"average_pb_total = \",loss_pb_total/9\n",
    "print 'Visualization'\n",
    "N = 1\n",
    "x = np.linspace(0, 336, 336)\n",
    "oneday_x = x\n",
    "vals = [1,2,3,4] # Values to iterate over and add/subtract from y.\n",
    "pl.rc('font', family='serif')\n",
    "fig = pl.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('The x values')\n",
    "ax.set_ylabel('The y values')\n",
    "#ax.set_ylim([0,1.8])\n",
    "pl.rc('font', family = 'serif', serif = 'Times')\n",
    "pl.rc('xtick', labelsize = 8)\n",
    "pl.rc('ytick', labelsize = 8)\n",
    "pl.rc('axes', labelsize = 8)\n",
    "#################################### one day case\n",
    "#for i, val in enumerate(vals):\n",
    "#    alpha = 0.5*(i+1)/len(vals) # Modify the alpha value for each iteration.\n",
    "#    if i == 0:\n",
    "#        ax.fill_between(oneday_x, Plist[8][0:336], Plist[0][0:336], color='red', alpha=alpha*0.7)\n",
    "#    elif i == 1:\n",
    "#        ax.fill_between(oneday_x, Plist[7][0:336], Plist[1][0:336], color='red', alpha=alpha*0.7)\n",
    "#    elif i == 2:\n",
    "#        ax.fill_between(oneday_x, Plist[6][0:336], Plist[2][0:336], color='red', alpha=alpha*0.7)\n",
    "#    else:\n",
    "#        ax.fill_between(oneday_x, Plist[5][0:336], Plist[3][0:336], color='red', alpha=alpha*0.7)\n",
    "boxP = np.array(Plist)\n",
    "boxP = boxP.T\n",
    "boxP = boxP.reshape([7,-1])\n",
    "boxP = boxP.T\n",
    "pd.DataFrame(boxP).boxplot()\n",
    "ax.plot(oneday_x/48+0.5, samples_res[0][0:336], '-', color='black',linewidth=0.8) # Plot the original signal\n",
    "\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
